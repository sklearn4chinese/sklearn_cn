##1.9   朴素贝叶斯

朴素贝叶斯方式是一系列监督算法的集合。这些监督算法基于应用贝叶斯的理论，即假设任意两个特征之间是独立无关的。假设一个类变量y和独立无关的特征向量x1~xn,贝叶斯理论呈现了如下的方法：
img
使用朴素假设，则有：
img
对于所有的i,可以简化成：
img
由于P(x1,...,xn)是输入常量，我们可以使用如下的分类方法：
img
然后可以使用最大化后验概率的方法来统计P(y)和P(xi|y)。前者就是在训练集中类y的频率。
不同的朴素贝叶斯分类器的区别在于他们对P(x|y)分布的假设不同。

尽管这个假设明显很简单，朴素贝叶斯分类器在许多现实世界中表现的非常好，比较有名的如文档分类器和垃圾邮件过滤。他们都需要很少的训练数据来统计所需的参数（至于为什么朴素贝叶斯方法在理论上为什么表现这么好，它主要作用于哪个类型，可以参看下面的附录）

朴素贝叶斯的学习和分类跟其他复杂的方法比起来，会非常的快。类条件特征分布的解耦意味着每个分布可以作为一个一维的分布单独去统计。这反过来又减轻了多维方法带来的问题。

从另一方面来说，尽管朴素贝叶斯方法是个很好的分类器，但是它同时也是一个比较差的统计器，所以通过predict_proba差生的概率值不要太当真。

###1.9.1 Gaussian Naive Bayes
GaussianNB 实现了高斯朴素贝叶斯方法来进行分类。特征的似然函数假设为高斯函数：
img

参数？？和？？使用最大化似然函数来统计。

###1.9.2  多项式朴素贝叶斯
MultinomialNB实现了多项式分布的贝叶斯算法，这是两个在文本分类中，两个经典的朴素贝叶斯变量中的一个（数据通常表示为词向量的数量，尽管tf-idf向量通常在实际中表现的更好）。对每个类y,分布由向量theata来参数化。n是参数的数量（在文本分类中，词的数量） thearayi是特征i的概率p（xi|y)在属于y的抽样点中。

参数theatay被统计为最大似然函数的平滑版本，例如通常概率统计为
img

平滑参数alpha


###1.9.3  伯努利朴素贝叶斯

bernuliNB实现了对于多重伯努利分布数据的训练和分类算法。比如，可能有多个特征值，但是每个特征值都是一个bool值。因此，这些类需要将样本点转化为布尔值的特征向量。如果有其他的值被处理，那么bernuliNB可能就会把他们转化为布尔值。
伯努利朴素贝叶斯的决策规则基于

与多项式朴素贝叶斯不同的地方在于，他明显的对哪些没有出现，但是可以预测的y的特征进行了惩罚，而多项式朴素贝叶斯知识忽略了他们。

在文本分类中，单词出现向量（不是单词次数向量）可能会被用于训练，用bernuliNB分类器可能在某些数据集上会表现的更好，尤其在短文本上。如果时间允许的话，建议两者都试一下。

###1.9.4  
当训练集无法全部装进内存时，朴素贝叶斯方法可以用于解决大数据分类的问题。为了解决这个问题，MultinomialNB，BernoulliNB，和GaussuanNB实现了一个partial_fit方法，它可以逐步的跟其他在文本分类的outofcore方法中证明有效的分类器一起工作。
跟拟合方法不同，首次调用partial_fit需要传入所有预期的类标签。
在scikit-learn中相关方法的目录中，也可以去看out-of-core的相关文档。




