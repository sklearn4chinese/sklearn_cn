##1.7 高斯处理
Gaussian Processes for Machine Learning (GPML)是一种通用的用于解决回归问题的监督算法。它也被应用到概率分类问题上，但是在目前的实现中，这只是一个回归练习的后处理过程。
在机器学习中，高斯预处理的优势有：
* 预测值会影响观测值（起码对普通的相关模型是这样的）
* 预测是概率形式的，因此可以用于计算置信区间和超越概率，这些都可以用于在某些领域进行重新拟合（在线拟合，自适应拟合）
* 通用性：不同的线性回归模型和相关模型都可以被指定。它提供了通用的模型，如果是固定的话，也可以指定一些自定义模型。

劣势有：
* 它不是稀疏的。它使用所有的样本点和特征来进行预测。
* 当在高维空间时，它会丧失有效性。也就是说，当特征数较多时，它可能会有较差的表现，损失计算的效率。
* 分类只是一个后验处理，意味着首先需要解决回归问题，通过提供所要拟合的y的完整的标量浮点精度。

由于该预测方法的高斯性质，它有着多种多样的应用。比如全局优化，概率分类等。

###1.7.1 例子
####1.7.1.1 一个介绍性的回归的例子
假设我们想要得到函数![1.7.1.1   ](http://scikit-learn.org/stable/_images/math/36ac45ff45741e9cb7933cec0ddf9731775f2d6e.png)。为了做到这点，我们设计了一个实验。我们定义了高斯过程模型，它的回归和相关模型可以通过额外的参数来指定，然后用模型来拟合这些数据。依赖于实例中提供的参数的数量，拟合过程可以通过最大化似然函数的方法来调整参数，或者通过指定参数的方法来进行训练。
![1.7.1.1](http://scikit-learn.org/stable/_images/plot_gp_regression_0011.png)
> 
\>>> import numpy as np
\>>> from sklearn import gaussian_process
\>>> def f(x):
...     return x * np.sin(x)
\>>> X = np.atleast_2d([1., 3., 5., 6., 7., 8.]).T
\>>> y = f(X).ravel()
\>>> x = np.atleast_2d(np.linspace(0, 10, 1000)).T
\>>> gp = gaussian_process.GaussianProcess(theta0=1e-2, thetaL=1e-4, thetaU=1e-1)
\>>> gp.fit(X, y)  
GaussianProcess(beta0=None, corr=<function squared_exponential at 0x...>,
        normalize=True, nugget=array(2.22...-15),
        optimizer='fmin_cobyla', random_start=1, random_state=...
        regr=<function constant at 0x...>, storage_mode='full',
        theta0=array([[ 0.01]]), thetaL=array([[ 0.0001]]),
        thetaU=array([[ 0.1]]), verbose=False)
\>>> y_pred, sigma2_pred = gp.predict(x, eval_MSE=True)

####1.7.1.2  拟合噪音数据
当包括噪音在内的数据进行拟合时，通过指定每个点的噪音的方差，就可以使用高斯过程模型。GaussianProcess使用了参数nugget，这个参数被加在了
训练样本点相关矩阵的对角上。通常这是一种Tikhonov 正则化的方式，在平方指数相关模型的特殊情况下，这种正则化等价于在输入中指定一个分数方差，那就是

![1.7.1.2](http://scikit-learn.org/stable/_images/math/b0997a6b918f5f8fc9a3615f1c8cb2d514a09296.png)

nugget和corr合理的设置后，高斯过程可以很好的从噪音中拟合函数。

![1.7.1.2](http://scikit-learn.org/stable/_images/plot_gp_regression_0021.png)
###1.7.2  数学公式
####1.7.2.1 最初的假设
假设一个人想要用模型来拟合一个计算机实验的输出，数学公式为

![](http://scikit-learn.org/stable/_images/math/7393ae52f899dd6a031b20cac4e675cd42489021.png)

GMML假定，这个函数是高斯过程的条件样本路径，高斯过程G如下式：

![](http://scikit-learn.org/stable/_images/math/9a43491764462185f82bac570ab9980a5a7ca43d.png)

其中，![](http://scikit-learn.org/stable/_images/math/df2224db0152d27d3a57574c262924c05d06da0f.png)是一个线性回归模型，![](http://scikit-learn.org/stable/_images/math/b8b7b5e99e95c584ec8d88d0bcf69748bfe4554a.png)是一个零均值的高斯过程，它有固定的方差函数：

![](http://scikit-learn.org/stable/_images/math/890afd90975e17be820ce96af42fb3eb2a466d0c.png)

![](http://scikit-learn.org/stable/_images/math/93c29bfedc993de115781b4fbb7e3a8b9e8d6b51.png)是方差，![](http://scikit-learn.org/stable/_images/math/9d86170e7de539c0ff999de09621ee0c7b6c8ed0.png)是相关函数，它只依赖于每个样本点间的相对距离，可能是特征明确的。
从这个基本的公式来看，GPML只不过是最小二乘线性回归问题的扩展。

![](http://scikit-learn.org/stable/_images/math/ef45beaabfa8af02547f3cc68b4cb6c812bbb73b.png)

除了我们假设相关函数提到的相关样本点之间的相关性。普通最小二乘法模型![](http://scikit-learn.org/stable/_images/math/9f6c60055c315ac425554bb304f775812a0710b9.png)是一个狄拉克相关模型，有时在kriging文献中被称为nugget相关模型，在![](http://scikit-learn.org/stable/_images/math/71faab2506ab54143f7deda2b7b00c75a836b5b8.png)的情况下。
####1.7.2.2最好的无偏预测
我们在观测值的条件下，对样本路径g的最好的线性无偏估计进行引申：

![](http://scikit-learn.org/stable/_images/math/d18ddf7dd7ca825898bbc0e7249228526f6bddea.png)

它出自以下性质：
* 它是线性的（是观测值的线性组合）

![](http://scikit-learn.org/stable/_images/math/261e8d3b761ecfa15d0e95734bc90afd76b1b29c.png)

* 它是无偏的

![](http://scikit-learn.org/stable/_images/math/64538044ea7c00385abadbb99a5a97a9a18a94cc.png)

* 它是最好的(从均方误差的角度来说）

![](http://scikit-learn.org/stable/_images/math/d976ecc77265e689f9fbe475c7464b285ede0acc.png)

所以最优的向量![](http://scikit-learn.org/stable/_images/math/5e857aeb235a55d0250954f6f71aef2c356db1ed.png)是下列最优化问题的等价式：

![](http://scikit-learn.org/stable/_images/math/dd45b04e42790a1f53f220a2e6c8d716578c922e.png)

用拉格朗日的形式重写这个约束优化问题，并进一步寻找能够满足一阶最优条件的解，最终为预测器找到一个最终的表达形式。可以在索引中查看更多的证明。
在最后，BLUP展示为高斯随机变量，均值为：

![](http://scikit-learn.org/stable/_images/math/aa59228cc55fc5e2d52dc1909bfa1054b4d0d9f7.png)

方差为：

![](http://scikit-learn.org/stable/_images/math/60dd23f78766f18d8e638f45e2d4f6643b2f31df.png)

我们之前介绍的：
* 相关矩阵的形式定义为自相关函数和参数![](http://scikit-learn.org/stable/_images/math/a9cfbeb8ebee1f365919e147a79e242dcb67ee5d.png)

![](http://scikit-learn.org/stable/_images/math/6cfceacfbcb485240500ae7ec7c12a7a0c45a2ee.png)

* 预测值的点和在DOE中的点组成的交叉相关向量。

![](http://scikit-learn.org/stable/_images/math/20f6d07c95bbe8ddd59f1ed7bcdc3fbfd21b5559.png)

* 回归矩阵
![](http://scikit-learn.org/stable/_images/math/30d44ed6819278c2c798e35370b8aef1dac18924.png)

* 最小二乘回归权重


* 向量

注意到高斯过程的概率值是完全解析，并且大多依赖于基本的线性运算，这点很重要。更精确的平均预测是两个简单的线性组合的总和，并且方差需要矩阵求逆，但是相关矩阵只能使用 Cholesky分解算法一次。










####1.7.2.3 经验最好的无偏预测

到现在为止，自相关和回归模型都是假定给出的。然而在现实中，他们不会被预先知道。因此，人们必须首先根据经验（或者动机）对这些相关模型进行选择。模型选择之后，还必须统计BLUP所涉及的未知参数。为了做到这点，需要使用一些观察集与推断相结合的方法。本实现是基于DACE matlab工具箱，使用最大似然估计法来进行的实现。可以看DACE手册看完整的公式引用。最大似然估计转化成全局最优的方法中的自相关参数。在本实现中，全局优化通过scipy.optimize中的fmin_cobyla优化函数的均值来解决。在各向异性的情况下，但是，我们提供了Welch的分量共优化算法的实现 - 请参阅参考资料。
对于高斯过程机器学习方面的理论更全面的描述，请参阅下面的参考资料：


###1.7.3  相关模型
一些通用的相关模型跟一些有名的SVM核是相通的，以为他们大部分建立于等价的假设上。他们必须满足Mercer条件，而且应该保持恒定。但是注意，相关模型的选择，必须跟产生观测值的原始实验的已知性质保持一致。例如，如果原始实验已知是无限可微（平滑）的，那么应该使用平方指数相关性模型；如果不是，那就应该只使用指数模型。
还要注意的是，有些相关性模型使用变异度作为输入，他就是Matern相关性模型。但是在这里没有实现它（TODO）
更详细的对相关性模型的选择的讨论，请参见参考中拉斯穆森和威廉姆斯的著作

###1.7.4 回归模型
常见的回归模型包括零（恒定）、一阶、二阶多项式。但是，人们可以指定自己的一个Python函数，它的功能x作为输入，并返回含有该功能集的值的向量形式。唯一的限制是，函数的数量不得超过可观测的数量，使得潜在的回归问题不会发生。

###1.7.5 实现细节   
目前的实现是基于DACE matlab的工具箱





